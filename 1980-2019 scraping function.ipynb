{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://en.wikipedia.org/wiki/List_of_American_films_of_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_after_1996_1(year):\n",
    "    #create the url \n",
    "    url = 'http://en.wikipedia.org/wiki/List_of_American_films_of_'+ str(year)\n",
    "    html = urlopen(url) \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    website_url = requests.get(url).text \n",
    "    soup = BeautifulSoup(website_url, 'lxml')\n",
    "    My_table = soup.find('table',{'class':'wikitable sortable'})\n",
    "    titles = []\n",
    "    directors = []\n",
    "    actors = []\n",
    "    genres = []\n",
    "    rows = My_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells)>=1:\n",
    "            title = cells[0]\n",
    "            titles.append(title.text)\n",
    "            director = cells[1]\n",
    "            directors.append(director.text)\n",
    "            actor = cells[2]\n",
    "            actors.append(actor.text)\n",
    "            genre = cells[3]\n",
    "            genres.append(genre.text)\n",
    "            index = [*range(1,len(titles)+1,1)]\n",
    "            df = pd.DataFrame({'Titles': titles,'Directors' : directors, 'Actors': actors, 'Genres': genres, }, index = index   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'timeout'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b91958f48d7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_1997\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_table_after_1996\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1997\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf_1997\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0d207ba6db6d>\u001b[0m in \u001b[0;36mget_table_after_1996\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_table_after_1996\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m    \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'html.parser'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0mwebsite_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m    \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebsite_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/learn-env/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    515\u001b[0m                 \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m         \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'timeout'"
     ]
    }
   ],
   "source": [
    "df_1997 = get_table_after_1996(1997)\n",
    "df_1997.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_before_1996(url):\n",
    "    html = urlopen(url) \n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    website_url = requests.get(url).text \n",
    "    soup = BeautifulSoup(website_url, 'lxml')\n",
    "    My_table = soup.find('table',{'class':'wikitable'})\n",
    "    titles = []\n",
    "    directors = []\n",
    "    actors = []\n",
    "    genres = []\n",
    "    rows = My_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cells = row.find_all('td')\n",
    "        if len(cells)>=1:\n",
    "            title = cells[0]\n",
    "            titles.append(title.text)\n",
    "            director = cells[1]\n",
    "            directors.append(director.text)\n",
    "            actor = cells[2]\n",
    "            actors.append(actor.text)\n",
    "            genre = cells[3]\n",
    "            genres.append(genre.text)\n",
    "            index = [*range(1,len(titles)+1,1)]\n",
    "    return pd.DataFrame({'Titles': titles,'Directors' : directors, 'Actors': actors, 'Genres': genres, }, index = index   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_after_1996_before_2014(url):\n",
    "    dfs = pd.read_html('https://en.wikipedia.org/wiki/List_of_American_films_of_2014', flavor=\"lxml\")\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'wikitable'})\n",
    "    dfs = pd.read_html(str(table).replace(\"2;\", \"2\"))\n",
    "    #df = dfs[0].drop(['Opening', 'Opening.1', 'Notes', 'Ref.'], axis = 1)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-b4ba2fa0b5dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "get_table_after_1996_before_2014(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_American_films_of_2014'\n",
    "# dfs = pd.read_html(https://en.wikipedia.org/wiki/List_of_American_films_of_2014, flavor=\"lxml\")\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "dfs = pd.read_html(str(table).replace(\"2;\", \"2\"))\n",
    "df = dfs[0].drop(['Opening', 'Opening.1', 'Notes', 'Ref.'], axis = 1)\n",
    "df.head()\n",
    "#def get_table_after_1996_before_2014(url):\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_American_films_of_2014'\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "dfs = pd.read_html(str(table).replace(\"5;\", \"5\"))\n",
    "len(dfs)\n",
    "#df_2014 = dfs[1].drop(['Opening', 'Opening.1', 'Notes', 'Ref.'], axis = 1)\n",
    "#return dfs\n",
    "#df_2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_after_1996_before_2014(url):\n",
    "# dfs = pd.read_html(https://en.wikipedia.org/wiki/List_of_American_films_of_2014, flavor=\"lxml\")\n",
    "   r = requests.get(url)\n",
    "   soup = BeautifulSoup(r.text, 'html.parser')\n",
    "   table = soup.find('table', {'class': 'wikitable sortable'})\n",
    "   dfs = pd.read_html(str(table).replace(\"2;\", \"2\"))\n",
    "   df = dfs[0].drop(['Opening', 'Opening.1', 'Notes', 'Ref.'], axis = 1)\n",
    "return df\n",
    "df_2014 = get_table_after_1996_before_2014('https://en.wikipedia.org/wiki/List_of_American_films_of_2014')\n",
    "df_2014\n",
    "df_1999 = get_table_after_1996('https://en.wikipedia.org/wiki/List_of_American_films_of_1999')\n",
    "df_1999.head()\n",
    "pieces = []\n",
    "for year in [*range(1950, 2020,1)]:\n",
    "    url = 'http://en.wikipedia.org/wiki/List_of_American_films_of_'+ str(year)\n",
    "    if year <= 1996:\n",
    "        df = get_table_before_1996(url)\n",
    "    elif year > 1996 < 2014:\n",
    "        df = get_table_after_1996(url)\n",
    "    else:\n",
    "        df = get_table_after_1996_before_2014(url)\n",
    "    pieces.append(df)\n",
    "return pieces\n",
    "df_all = pd.concat(pieces)\n",
    "df_all\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
